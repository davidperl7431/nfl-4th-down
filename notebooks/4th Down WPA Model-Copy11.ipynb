{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a449ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x161c2bc4bd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import optuna\n",
    "\n",
    "# modeling\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import logit, expit\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# PyTorch for conversion model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# nfl pbp loader\n",
    "import nfl_data_py as nfl\n",
    "\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f038186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading play-by-play for seasons: range(2011, 2026)\n",
      "2011 done.\n",
      "2012 done.\n",
      "2013 done.\n",
      "2014 done.\n",
      "2015 done.\n",
      "2016 done.\n",
      "2017 done.\n",
      "2018 done.\n",
      "2019 done.\n",
      "2020 done.\n",
      "2021 done.\n",
      "2022 done.\n",
      "2023 done.\n",
      "2024 done.\n",
      "2025 done.\n",
      "Rows loaded: 722172\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>play_id</th>\n",
       "      <th>game_id</th>\n",
       "      <th>old_game_id</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>season_type</th>\n",
       "      <th>week</th>\n",
       "      <th>posteam</th>\n",
       "      <th>posteam_type</th>\n",
       "      <th>defteam</th>\n",
       "      <th>...</th>\n",
       "      <th>was_pressure</th>\n",
       "      <th>route</th>\n",
       "      <th>defense_man_zone_type</th>\n",
       "      <th>defense_coverage_type</th>\n",
       "      <th>offense_names</th>\n",
       "      <th>defense_names</th>\n",
       "      <th>offense_positions</th>\n",
       "      <th>defense_positions</th>\n",
       "      <th>offense_numbers</th>\n",
       "      <th>defense_numbers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2011_01_ATL_CHI</td>\n",
       "      <td>2011091105</td>\n",
       "      <td>CHI</td>\n",
       "      <td>ATL</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>2011_01_ATL_CHI</td>\n",
       "      <td>2011091105</td>\n",
       "      <td>CHI</td>\n",
       "      <td>ATL</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>CHI</td>\n",
       "      <td>home</td>\n",
       "      <td>ATL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.0</td>\n",
       "      <td>2011_01_ATL_CHI</td>\n",
       "      <td>2011091105</td>\n",
       "      <td>CHI</td>\n",
       "      <td>ATL</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>CHI</td>\n",
       "      <td>home</td>\n",
       "      <td>ATL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>2011_01_ATL_CHI</td>\n",
       "      <td>2011091105</td>\n",
       "      <td>CHI</td>\n",
       "      <td>ATL</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>CHI</td>\n",
       "      <td>home</td>\n",
       "      <td>ATL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>112.0</td>\n",
       "      <td>2011_01_ATL_CHI</td>\n",
       "      <td>2011091105</td>\n",
       "      <td>CHI</td>\n",
       "      <td>ATL</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>CHI</td>\n",
       "      <td>home</td>\n",
       "      <td>ATL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   play_id          game_id old_game_id home_team away_team season_type  week  \\\n",
       "0      1.0  2011_01_ATL_CHI  2011091105       CHI       ATL         REG     1   \n",
       "1     36.0  2011_01_ATL_CHI  2011091105       CHI       ATL         REG     1   \n",
       "2     69.0  2011_01_ATL_CHI  2011091105       CHI       ATL         REG     1   \n",
       "3     91.0  2011_01_ATL_CHI  2011091105       CHI       ATL         REG     1   \n",
       "4    112.0  2011_01_ATL_CHI  2011091105       CHI       ATL         REG     1   \n",
       "\n",
       "  posteam posteam_type defteam  ... was_pressure  route defense_man_zone_type  \\\n",
       "0    None         None    None  ...          NaN    NaN                   NaN   \n",
       "1     CHI         home     ATL  ...          NaN    NaN                   NaN   \n",
       "2     CHI         home     ATL  ...          NaN    NaN                   NaN   \n",
       "3     CHI         home     ATL  ...          NaN    NaN                   NaN   \n",
       "4     CHI         home     ATL  ...          NaN    NaN                   NaN   \n",
       "\n",
       "   defense_coverage_type  offense_names  defense_names offense_positions  \\\n",
       "0                    NaN            NaN            NaN               NaN   \n",
       "1                    NaN            NaN            NaN               NaN   \n",
       "2                    NaN            NaN            NaN               NaN   \n",
       "3                    NaN            NaN            NaN               NaN   \n",
       "4                    NaN            NaN            NaN               NaN   \n",
       "\n",
       "   defense_positions  offense_numbers  defense_numbers  \n",
       "0                NaN              NaN              NaN  \n",
       "1                NaN              NaN              NaN  \n",
       "2                NaN              NaN              NaN  \n",
       "3                NaN              NaN              NaN  \n",
       "4                NaN              NaN              NaN  \n",
       "\n",
       "[5 rows x 398 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download PBP\n",
    "seasons = range(2011,2026)\n",
    "print(\"Loading play-by-play for seasons:\", seasons)\n",
    "raw_pbp = nfl.import_pbp_data(seasons, downcast=False)  # returns a DataFrame (likely large)\n",
    "\n",
    "print(\"Rows loaded:\", raw_pbp.shape[0])\n",
    "raw_pbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02287f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weather(weather_str):\n",
    "    \"\"\"\n",
    "    Parses a weather string into structured features:\n",
    "        - temp_F: float\n",
    "        - humidity: float (percentage)\n",
    "        - wind_mph: float\n",
    "        - wind_dir: str\n",
    "        - conditions: str (general description, e.g., 'sunny', 'cloudy', etc.)\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        \"temp_F\": None,\n",
    "        \"humidity\": None,\n",
    "        \"wind_mph\": None,\n",
    "        \"wind_dir\": None,\n",
    "        \"conditions\": None\n",
    "    }\n",
    "    \n",
    "    if not isinstance(weather_str, str):\n",
    "        return result\n",
    "    \n",
    "    lower_str = weather_str.lower()\n",
    "    \n",
    "    # Extract temperature\n",
    "    temp_match = re.search(r'(\\d+)\\s*°?\\s*f', lower_str)\n",
    "    if temp_match:\n",
    "        result['temp_F'] = float(temp_match.group(1))\n",
    "    \n",
    "    # Extract humidity\n",
    "    hum_match = re.search(r'humidity[:\\s]*(\\d+)%', lower_str)\n",
    "    if hum_match:\n",
    "        result['humidity'] = float(hum_match.group(1))\n",
    "    \n",
    "    # Extract wind speed and direction\n",
    "    wind_match = re.search(r'wind[:\\s]*([nesw]+)\\s*(\\d+)\\s*mph', lower_str)\n",
    "    if wind_match:\n",
    "        result['wind_dir'] = wind_match.group(1).upper()\n",
    "        result['wind_mph'] = float(wind_match.group(2))\n",
    "    \n",
    "    # Extract general conditions\n",
    "    conditions = []\n",
    "    for cond in ['sunny', 'cloudy', 'clear', 'rain', 'snow', 'fog', 'drizzle', 'storm', 'windy']:\n",
    "        if cond in lower_str:\n",
    "            conditions.append(cond)\n",
    "    if conditions:\n",
    "        result['conditions'] = ','.join(conditions)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def deconstruct_weather(df, weather_col='weather'):\n",
    "    \"\"\"\n",
    "    Adds structured weather columns to a DataFrame based on a weather string column.\n",
    "    \n",
    "    New columns added:\n",
    "      - temp_F\n",
    "      - humidity\n",
    "      - wind_mph\n",
    "      - wind_dir\n",
    "      - conditions\n",
    "    \"\"\"\n",
    "    weather_data = df[weather_col].apply(parse_weather)\n",
    "    weather_df = pd.DataFrame(weather_data.tolist())\n",
    "    df = pd.concat([df.reset_index(drop=True), weather_df], axis=1)\n",
    "    \n",
    "    # Fill missing wind speeds with 0\n",
    "    df['wind_mph'] = df['wind_mph'].fillna(0)\n",
    "\n",
    "    # Fill missing temperatures with 60°F\n",
    "    df['temp_F'] = df['temp_F'].fillna(60)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6062feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['play_type', 'season', 'home_wp_post', 'away_wp_post', 'weather', 'yardline_100', 'ydstogo',\n",
    "               'game_seconds_remaining', 'half_seconds_remaining', 'posteam', 'defteam',\n",
    "               'posteam_timeouts_remaining', 'defteam_timeouts_remaining', 'kick_distance', 'touchback',\n",
    "                'return_yards', 'first_down', 'touchdown', 'game_id', 'score_differential',\n",
    "                'home_team', 'away_team', 'home_score', 'away_score', 'down', 'field_goal_result', 'penalty',\n",
    "               'home_coach', 'away_coach', 'spread_line', 'total_line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159098a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp = raw_pbp.loc[:, cols_to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3da8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_col = {\n",
    "    \"punt\": \"punt\",\n",
    "    \"field_goal\": \"field_goal\",\n",
    "    \"run\": \"go\",\n",
    "    \"pass\": \"go\"\n",
    "}\n",
    "\n",
    "pbp[\"play_type_actual\"] = pbp[\"play_type\"].map(action_to_col)\n",
    "pbp = pbp[pbp.play_type_actual.isin(['punt', 'go', 'field_goal'])]\n",
    "pbp = deconstruct_weather(pbp)\n",
    "pbp = pbp[pbp.penalty == 0]\n",
    "pbp['fg_made'] = (pbp[\"field_goal_result\"] == \"made\").astype(int)\n",
    "\n",
    "action_to_ewp_col = {\n",
    "    \"punt\": \"ewp_punt\",\n",
    "    \"field_goal\": \"ewp_fg\",\n",
    "    \"go\": \"ewp_go\"\n",
    "}\n",
    "pbp[\"actual_ewp_col\"] = pbp[\"play_type_actual\"].map(action_to_ewp_col)\n",
    "\n",
    "pbp[\"possession_coach\"] = np.where(\n",
    "    pbp[\"posteam\"] == pbp[\"home_team\"],\n",
    "    pbp[\"home_coach\"],\n",
    "    pbp[\"away_coach\"]\n",
    ")\n",
    "\n",
    "pbp[\"defending_coach\"] = np.where(\n",
    "    pbp[\"posteam\"] == pbp[\"home_team\"],\n",
    "    pbp[\"away_coach\"],\n",
    "    pbp[\"home_coach\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e7bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = pbp.season.unique() # seasons\n",
    "test_season = seasons.max()\n",
    "\n",
    "pbp_train = pbp[pbp.season != test_season]\n",
    "pbp_test = pbp[pbp.season == test_season]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5683fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_temporal_folds(df, season_col=\"season\", min_train_seasons=3):\n",
    "    \"\"\"\n",
    "    Expanding-window CV folds by season.\n",
    "    Returns list of (train_idx, val_idx).\n",
    "    \"\"\"\n",
    "    seasons = np.sort(df[season_col].unique())\n",
    "    folds = []\n",
    "\n",
    "    for i in range(min_train_seasons, len(seasons)):\n",
    "        train_seasons = seasons[:i]\n",
    "        val_season = seasons[i]\n",
    "\n",
    "        train_idx = df[df[season_col].isin(train_seasons)].index\n",
    "        val_idx = df[df[season_col] == val_season].index\n",
    "\n",
    "        folds.append((train_idx, val_idx))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a81eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Drop rows missing home/away WP\n",
    "wp_df = pbp_train.dropna(subset=[\"home_wp_post\", \"away_wp_post\"]).copy()\n",
    "\n",
    "# --- Define features\n",
    "wp_df[\"score_time_ratio\"] = wp_df[\"score_differential\"].abs() / (wp_df[\"game_seconds_remaining\"] + 1)\n",
    "wp_features = [\n",
    "    \"yardline_100\",\n",
    "    \"down\",\n",
    "    \"ydstogo\",\n",
    "    \"game_seconds_remaining\",\n",
    "    \"half_seconds_remaining\",\n",
    "    \"score_differential\",\n",
    "    \"posteam_timeouts_remaining\",\n",
    "    \"defteam_timeouts_remaining\",\n",
    "    \"temp_F\",\n",
    "    \"wind_mph\",\n",
    "    \"spread_line\",\n",
    "    \"total_line\"\n",
    "]\n",
    "\n",
    "# --- Define posteam WP target\n",
    "wp_df[\"wp_target\"] = np.where(\n",
    "    wp_df[\"posteam\"] == wp_df[\"home_team\"],\n",
    "    wp_df[\"home_wp_post\"],\n",
    "    wp_df[\"away_wp_post\"]\n",
    ")\n",
    "\n",
    "wp_df = wp_df.reset_index(drop=True)\n",
    "\n",
    "X_wp = wp_df[wp_features]\n",
    "y_wp = wp_df[\"wp_target\"]\n",
    "\n",
    "# --- Clip target to avoid exact 0/1 ---\n",
    "epsilon = 1e-6\n",
    "y_wp_clipped = y_wp.clip(epsilon, 1 - epsilon).reset_index(drop=True)\n",
    "\n",
    "# --- Monotone constraints\n",
    "monotone_constraints_dict = {\n",
    "    \"yardline_100\": -1,               # closer to opponent endzone → WP ↑\n",
    "    \"down\": -1,                       # higher down (worse) → WP ↓\n",
    "    \"ydstogo\": -1,                    # more yards to go → WP ↓\n",
    "    \"score_differential\": 1,          # lead → WP ↑\n",
    "    \"posteam_timeouts_remaining\": 1,  # more TOs → WP ↑\n",
    "    \"defteam_timeouts_remaining\": -1  # opponent TOs → WP ↓\n",
    "}\n",
    "\n",
    "mono_tuple = tuple(monotone_constraints_dict.get(c, 0) for c in X_wp.columns)\n",
    "\n",
    "wp_folds = make_temporal_folds(wp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c09fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_objective(trial):\n",
    "    \n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.08),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 400),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.9),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 20, 100),\n",
    "        \"verbosity\": 0,\n",
    "        \"monotone_constraints\": mono_tuple,\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"early_stopping_rounds\": 100,\n",
    "        \"max_bin\": 128,\n",
    "        \"n_jobs\": 4,\n",
    "    }\n",
    "\n",
    "    rmses = []\n",
    "    for train_idx, val_idx in wp_folds:\n",
    "        X_train = X_wp.iloc[train_idx].to_numpy(dtype=np.float32, copy=False)\n",
    "        X_val   = X_wp.iloc[val_idx].to_numpy(dtype=np.float32, copy=False)\n",
    "        y_train = y_wp_clipped.iloc[train_idx].to_numpy(dtype=np.float32, copy=False)\n",
    "        y_val   = y_wp_clipped.iloc[val_idx].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        rmses.append(mean_squared_error(y_val, preds, squared=False))\n",
    "\n",
    "    return float(np.mean(rmses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9264a522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-14 17:41:45,933]\u001b[0m Using an existing study with name 'wp_study' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2026-01-14 17:42:40,695]\u001b[0m Trial 3 finished with value: 0.062022995203733444 and parameters: {'max_depth': 5, 'learning_rate': 0.05226312413133263, 'n_estimators': 262, 'subsample': 0.8955649623211921, 'colsample_bytree': 0.6640539287151979, 'min_child_weight': 100}. Best is trial 2 with value: 0.06093452870845795.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "storage = \"sqlite:///optuna/wp_study.db\"\n",
    "\n",
    "wp_study = optuna.create_study(\n",
    "    study_name=\"wp_study\",\n",
    "    direction=\"minimize\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "wp_study.optimize(wp_objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c081be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE: 0.06093452870845795\n",
      "\n",
      "Best params: {'colsample_bytree': 0.8573280722200014, 'learning_rate': 0.06693800341131347, 'max_depth': 4, 'min_child_weight': 29, 'n_estimators': 378, 'subsample': 0.838526798409394}\n"
     ]
    }
   ],
   "source": [
    "wp_best_params = wp_study.best_params\n",
    "wp_best_score = wp_study.best_value\n",
    "\n",
    "print(\"Best CV RMSE:\", wp_best_score)\n",
    "print()\n",
    "print(\"Best params:\", wp_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "233ce880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add monotone constraints if not in params already\n",
    "wp_best_params[\"monotone_constraints\"] = monotone_constraints_dict\n",
    "wp_best_params[\"verbosity\"] = 0\n",
    "\n",
    "wp_model = XGBRegressor(**wp_best_params)\n",
    "wp_model.fit(X_wp, y_wp_clipped)  # Train on full dataset\n",
    "\n",
    "\n",
    "def predict_wp(state_df):\n",
    "    \"\"\"\n",
    "    Returns win probability for the team with possession in state_df.\n",
    "    \"\"\"\n",
    "    if \"score_time_ratio\" not in state_df.columns:\n",
    "        state_df = state_df.copy()  # prevents SettingWithCopyWarning\n",
    "        state_df.loc[:, \"score_time_ratio\"] = (\n",
    "            state_df[\"score_differential\"].abs() / (state_df[\"game_seconds_remaining\"] + 1)\n",
    "        )\n",
    "\n",
    "    preds = wp_model.predict(state_df[wp_features])\n",
    "    \n",
    "    return np.clip(preds, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04f264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wp_symmetric_adjust(state_df, predict_wp):\n",
    "    # Predict from original perspective\n",
    "    wp = predict_wp(state_df)\n",
    "\n",
    "    # Create flipped states\n",
    "    state_flipped = state_df.copy()\n",
    "    state_flipped[\"score_differential\"] *= -1\n",
    "    state_flipped[[\"posteam_timeouts_remaining\", \"defteam_timeouts_remaining\"]] = (\n",
    "        state_flipped[[\"defteam_timeouts_remaining\", \"posteam_timeouts_remaining\"]].values\n",
    "    )\n",
    "    state_flipped[\"yardline_100\"] = 100 - state_flipped[\"yardline_100\"]\n",
    "\n",
    "    # Only handle score_time_ratio if WP model actually uses it\n",
    "    if \"score_time_ratio\" in wp_features:\n",
    "        if \"score_time_ratio\" not in state_flipped.columns:\n",
    "            state_flipped.loc[:, \"score_time_ratio\"] = (\n",
    "                state_flipped[\"score_differential\"].abs() / (state_flipped[\"game_seconds_remaining\"] + 1)\n",
    "            )\n",
    "\n",
    "    wp_flipped = predict_wp(state_flipped)\n",
    "\n",
    "    # Symmetric adjustment\n",
    "    wp_sym = 0.5 * (wp + (1 - wp_flipped))\n",
    "\n",
    "    sym_weighting = 0.20\n",
    "    return (1 - sym_weighting) * wp + sym_weighting * wp_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab7ef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   feature  importance\n",
      "        score_differential    0.919909\n",
      "              yardline_100    0.038034\n",
      "    game_seconds_remaining    0.017918\n",
      "                      down    0.010859\n",
      "posteam_timeouts_remaining    0.004518\n",
      "                   ydstogo    0.004100\n",
      "    half_seconds_remaining    0.002688\n",
      "defteam_timeouts_remaining    0.001252\n",
      "               spread_line    0.000222\n",
      "                total_line    0.000198\n",
      "                  wind_mph    0.000153\n",
      "                    temp_F    0.000149\n"
     ]
    }
   ],
   "source": [
    "imp = (pd.DataFrame({\n",
    "        \"feature\": wp_features,\n",
    "        \"importance\": wp_model.feature_importances_\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "\n",
    "print(imp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5cb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create punt_df with only punt plays\n",
    "punt_df = pbp_train[pbp_train.play_type_actual == \"punt\"].dropna(subset=[\"kick_distance\", \"return_yards\"]).copy()\n",
    "\n",
    "# Compute net punt yardage: kick distance minus return yards, adjust for touchbacks (if available)\n",
    "# Assuming touchback puts ball at 20-yard line\n",
    "punt_df[\"net_punt\"] = punt_df[\"kick_distance\"] - punt_df[\"return_yards\"]\n",
    "punt_df.loc[punt_df[\"touchback\"] == 1, \"net_punt\"] = punt_df[\"yardline_100\"] - 20\n",
    "\n",
    "# Reset index to avoid any issues\n",
    "punt_df = punt_df.reset_index(drop=True)\n",
    "\n",
    "# Make temporal folds based on seasons in punt_df\n",
    "punt_folds = make_temporal_folds(punt_df)\n",
    "\n",
    "# Features to predict net punt\n",
    "punt_df[\"score_time_ratio\"] = punt_df[\"score_differential\"].abs() / (punt_df[\"game_seconds_remaining\"] + 1)\n",
    "punt_features = [\n",
    "    \"yardline_100\", \n",
    "    \"game_seconds_remaining\", \n",
    "    \"half_seconds_remaining\",\n",
    "    \"score_differential\",\n",
    "    \"posteam_timeouts_remaining\",\n",
    "    \"defteam_timeouts_remaining\",\n",
    "    \"temp_F\",\n",
    "    \"wind_mph\",\n",
    "    \"spread_line\",\n",
    "    \"total_line\"\n",
    "]\n",
    "\n",
    "X_punt = punt_df[punt_features].values\n",
    "y_punt = punt_df[\"net_punt\"].values\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_punt_scaled = X_scaler.fit_transform(X_punt)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_punt_scaled = y_scaler.fit_transform(y_punt.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296053db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punt_objective(trial):\n",
    "\n",
    "    # Hyperparameters\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "\n",
    "    max_epochs = 500\n",
    "    patience = 20\n",
    "    tol = 1e-4\n",
    "\n",
    "    rmses = []\n",
    "\n",
    "    for train_idx, val_idx in punt_folds:\n",
    "\n",
    "        X_train = torch.tensor(X_punt_scaled[train_idx], dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_punt_scaled[train_idx], dtype=torch.float32)\n",
    "        X_val   = torch.tensor(X_punt_scaled[val_idx], dtype=torch.float32)\n",
    "        y_val   = torch.tensor(y_punt_scaled[val_idx], dtype=torch.float32)\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim if i == 0 else hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        layers.append(nn.Linear(hidden_size, 1))\n",
    "\n",
    "        model = nn.Sequential(*layers)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        best_val = np.inf\n",
    "        wait = 0\n",
    "        best_state = None\n",
    "\n",
    "        # ---- training with early stopping ----\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_train)\n",
    "            loss = criterion(preds, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_val)\n",
    "                val_loss = criterion(val_preds, y_val).item()\n",
    "\n",
    "            if val_loss < best_val - tol:\n",
    "                best_val = val_loss\n",
    "                wait = 0\n",
    "                best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        # restore best model\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "        rmses.append(np.sqrt(best_val))\n",
    "\n",
    "    return np.mean(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f594ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-14 17:42:48,215]\u001b[0m Using an existing study with name 'punt_study' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2026-01-14 17:43:18,070]\u001b[0m Trial 2 finished with value: 0.9000922537913827 and parameters: {'n_layers': 1, 'hidden_size': 87, 'lr': 0.0018942425380153126, 'dropout': 0.38605082984662403}. Best is trial 1 with value: 0.8995588665425949.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "storage = \"sqlite:///optuna/punt_study.db\"\n",
    "\n",
    "punt_study = optuna.create_study(\n",
    "    study_name=\"punt_study\",\n",
    "    direction=\"minimize\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "punt_study.optimize(punt_objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f14bbb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV RMSE: 0.8995588665425949\n",
      "\n",
      "Best params: {'dropout': 0.0928213653222017, 'hidden_size': 76, 'lr': 0.0046027766667468585, 'n_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "punt_best_params = punt_study.best_params\n",
    "punt_best_score = punt_study.best_value\n",
    "n_layers = punt_best_params[\"n_layers\"]\n",
    "hidden_size = punt_best_params[\"hidden_size\"]\n",
    "dropout_rate = punt_best_params[\"dropout\"]\n",
    "lr = punt_best_params[\"lr\"]\n",
    "\n",
    "print(\"Best CV RMSE:\", punt_best_score)\n",
    "print()\n",
    "print(\"Best params:\", punt_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b01f731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final model\n",
    "layers = []\n",
    "input_dim = X_punt_scaled.shape[1]\n",
    "for i in range(n_layers):\n",
    "    layers.append(nn.Linear(input_dim if i==0 else hidden_size, hidden_size))\n",
    "    layers.append(nn.ReLU())\n",
    "    layers.append(nn.Dropout(dropout_rate))\n",
    "layers.append(nn.Linear(hidden_size, 1))\n",
    "punt_model = nn.Sequential(*layers)\n",
    "\n",
    "# Convert full data to tensors\n",
    "X_t = torch.tensor(X_punt_scaled, dtype=torch.float32)\n",
    "y_t = torch.tensor(y_punt_scaled, dtype=torch.float32)\n",
    "\n",
    "optimizer = torch.optim.Adam(punt_model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train final model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    preds = punt_model(X_t)\n",
    "    loss = loss_fn(preds, y_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12a28223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG (LogReg) OOF RMSE: 0.34176708351622065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Filter to field goal attempts only ---\n",
    "fg_df = pbp_train[pbp_train.play_type_actual == \"field_goal\"].dropna(subset=[\"field_goal_result\"]).copy()\n",
    "fg_df = fg_df[fg_df.field_goal_result.isin(['made', 'missed', 'blocked'])]\n",
    "\n",
    "# Field goal\n",
    "fg_df[\"score_time_ratio\"] = fg_df[\"score_differential\"].abs() / (fg_df[\"game_seconds_remaining\"] + 1)\n",
    "fg_features = [\n",
    "    \"yardline_100\",\n",
    "    \"game_seconds_remaining\",\n",
    "    \"half_seconds_remaining\",\n",
    "    \"score_differential\",\n",
    "    \"temp_F\",\n",
    "    \"wind_mph\",\n",
    "    \"spread_line\",\n",
    "    \"total_line\"\n",
    "]\n",
    "\n",
    "X_fg = fg_df[fg_features]\n",
    "y_fg = fg_df[\"fg_made\"]\n",
    "\n",
    "fg_folds = make_temporal_folds(fg_df)\n",
    "\n",
    "fg_oof_pred = pd.Series(index=fg_df.index, dtype=float)\n",
    "fg_df[\"fg_make_prob_oof\"] = fg_oof_pred\n",
    "\n",
    "for fold_num, (train_idx, val_idx) in enumerate(fg_folds, 1):\n",
    "    X_train = X_fg.loc[train_idx]\n",
    "    y_train = y_fg.loc[train_idx]\n",
    "    X_val   = X_fg.loc[val_idx]\n",
    "\n",
    "    fg_model_lr_fold = LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000\n",
    "    )\n",
    "\n",
    "    fg_model_lr_fold.fit(X_train, y_train)\n",
    "\n",
    "    fg_oof_pred.loc[val_idx] = fg_model_lr_fold.predict_proba(X_val)[:, 1]\n",
    "\n",
    "mask = fg_oof_pred.notna()\n",
    "fg_oof_rmse = np.sqrt(np.mean((fg_oof_pred[mask] - y_fg[mask]) ** 2))\n",
    "print(\"FG (LogReg) OOF RMSE:\", fg_oof_rmse)\n",
    "\n",
    "fg_model = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "fg_model.fit(X_fg, y_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6362be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fg = 65\n",
    "fg_decay_threshold = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfe992ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to 4th-down go-for-it plays\n",
    "go_df = pbp_train[\n",
    "    (pbp_train['down'] == 4) &\n",
    "    (pbp_train['play_type_actual'] == 'go')  # filters out punts/FGs\n",
    "].copy()\n",
    "\n",
    "# Target: did the team convert?\n",
    "go_df = go_df.dropna(subset=['first_down'])\n",
    "\n",
    "# Go-for-it conversion\n",
    "go_df[\"score_time_ratio\"] = go_df[\"score_differential\"].abs() / (go_df[\"game_seconds_remaining\"] + 1)\n",
    "\n",
    "go_df[\"success\"] = (\n",
    "    (go_df[\"first_down\"] == 1) |\n",
    "    (go_df[\"touchdown\"] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Reset index to avoid any issues\n",
    "go_df = go_df.reset_index(drop=True)\n",
    "\n",
    "# Make temporal folds based on seasons in punt_df\n",
    "go_folds = make_temporal_folds(go_df)\n",
    "\n",
    "# Features to predict net punt\n",
    "go_features = [\n",
    "    \"yardline_100\",\n",
    "    \"ydstogo\",\n",
    "    \"game_seconds_remaining\",\n",
    "    \"half_seconds_remaining\",\n",
    "    \"score_differential\",\n",
    "    \"posteam_timeouts_remaining\",\n",
    "    \"defteam_timeouts_remaining\",\n",
    "    \"temp_F\",\n",
    "    \"wind_mph\",\n",
    "    \"spread_line\",\n",
    "    \"total_line\"\n",
    "]\n",
    "\n",
    "X_go = go_df[go_features].values\n",
    "y_go = go_df[\"success\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "154b0801",
   "metadata": {},
   "outputs": [],
   "source": [
    "monotone_constraints = [\n",
    "    -1,  # yardline_100 (farther → worse)\n",
    "    -1,  # ydstogo (longer → worse)\n",
    "    0,   # game_seconds_remaining\n",
    "    0,   # half_seconds_remaining\n",
    "    1,   # score_differential\n",
    "    0,   # posteam_timeouts_remaining\n",
    "    0,   # defteam_timeouts_remaining\n",
    "    0,   # temp_F\n",
    "    0,   # wind_mph\n",
    "    1,   # spread_line\n",
    "    0,   # total_line\n",
    "]\n",
    "\n",
    "def go_objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"monotone_constraints\": tuple(monotone_constraints),\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"early_stopping_rounds\" : 100,\n",
    "        \"use_label_encoder\": False,\n",
    "        \"tree_method\": \"hist\",\n",
    "    }\n",
    "\n",
    "    log_losses = []\n",
    "\n",
    "    for train_idx, val_idx in go_folds:\n",
    "        X_train, X_val = X_go[train_idx], X_go[val_idx]\n",
    "        y_train, y_val = y_go[train_idx], y_go[val_idx]\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict_proba(X_val)[:, 1]\n",
    "        log_losses.append(log_loss(y_val, preds))\n",
    "\n",
    "    return np.mean(log_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a51eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-01-14 17:44:12,497]\u001b[0m Using an existing study with name 'go_study' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2026-01-14 17:44:29,724]\u001b[0m Trial 2 finished with value: 0.673603512047036 and parameters: {'max_depth': 5, 'learning_rate': 0.002526960128597689, 'n_estimators': 149, 'subsample': 0.9440393110618596, 'colsample_bytree': 0.7306901613444055}. Best is trial 1 with value: 0.6480236680252894.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "storage = \"sqlite:///optuna/go_study.db\"\n",
    "\n",
    "go_study = optuna.create_study(\n",
    "    study_name=\"go_study\",\n",
    "    direction=\"minimize\",\n",
    "    storage=storage,\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "go_study.optimize(go_objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3025543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8173038882244495, device=None,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric='logloss', feature_types=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.009203774084192835,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=(-1, -1, 0, 0, 1, 0, 0, 0, 0, 1, 0),\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train final model with best hyperparameters\n",
    "go_best_params = go_study.best_trial.params\n",
    "go_best_params[\"monotone_constraints\"] = tuple(monotone_constraints)\n",
    "go_best_params[\"use_label_encoder\"] = False\n",
    "go_best_params[\"eval_metric\"] = \"logloss\"\n",
    "\n",
    "# Compute class imbalance weight\n",
    "pos = (y_go == 1).sum()\n",
    "neg = (y_go == 0).sum()\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "go_model = XGBClassifier(**go_best_params, scale_pos_weight=scale_pos_weight)\n",
    "go_model.fit(X_go, y_go)  # feed raw features, no scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d6dca82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   feature  importance\n",
      "                   ydstogo    0.380500\n",
      "    half_seconds_remaining    0.084497\n",
      "    game_seconds_remaining    0.083226\n",
      "        score_differential    0.075728\n",
      "                total_line    0.068980\n",
      "posteam_timeouts_remaining    0.059966\n",
      "                    temp_F    0.052952\n",
      "              yardline_100    0.051203\n",
      "                  wind_mph    0.049787\n",
      "defteam_timeouts_remaining    0.048380\n",
      "               spread_line    0.044781\n"
     ]
    }
   ],
   "source": [
    "imp = (pd.DataFrame({\n",
    "        \"feature\": go_features,\n",
    "        \"importance\": go_model.feature_importances_\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    ")\n",
    "\n",
    "print(imp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c61343c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plays_df(df):\n",
    "    \n",
    "    # Compute final scores and win from offensive team perspective\n",
    "    final_scores = (\n",
    "        df.groupby(\"game_id\")\n",
    "           .tail(1)[[\"game_id\",\"home_team\",\"away_team\",\"home_score\",\"away_score\"]]\n",
    "           .copy()\n",
    "    )\n",
    "    final_scores[\"home_win\"] = (final_scores[\"home_score\"] > final_scores[\"away_score\"]).astype(int)\n",
    "\n",
    "    df = df.merge(final_scores[[\"game_id\",\"home_win\"]], on=\"game_id\", how=\"left\")\n",
    "    df[\"win_actual\"] = np.where(\n",
    "        df[\"posteam\"] == df[\"home_team\"],\n",
    "        df[\"home_win\"],\n",
    "        1 - df[\"home_win\"]\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0339def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_next_fg_conv_states(df):\n",
    "    \n",
    "    # Next state if successful field goal attempt\n",
    "    df['fg_success_yardline_100'] = 75\n",
    "    df['fg_success_down'] = 1\n",
    "    df['fg_success_ydstogo'] = 10\n",
    "    df['fg_success_game_seconds_remaining'] = np.maximum(0, df['game_seconds_remaining'] - 5)\n",
    "    df['fg_success_half_seconds_remaining'] = np.maximum(0, df['half_seconds_remaining'] - 5)\n",
    "    df['fg_success_score_differential'] = -(df['score_differential'] + 3)\n",
    "    df['fg_success_posteam_timeouts_remaining'] = df['defteam_timeouts_remaining']\n",
    "    df['fg_success_defteam_timeouts_remaining'] = df['posteam_timeouts_remaining']\n",
    "    df['fg_success_score_time_ratio'] = df['fg_success_score_differential'].abs() / (df['fg_success_game_seconds_remaining'] + 1)\n",
    "    df['fg_success_temp_F'] = df['temp_F']\n",
    "    df['fg_success_wind_mph'] = df['wind_mph']\n",
    "    df['fg_success_spread_line'] = df['spread_line']\n",
    "    df['fg_success_total_line'] = df['total_line']\n",
    "    \n",
    "    # Next state if failed field goal attempt\n",
    "    df['fg_fail_yardline_100'] = np.minimum(80, 100 - (df['yardline_100'] + 7)) # Account for inside 20-yardline edge case\n",
    "    df['fg_fail_down'] = 1\n",
    "    df['fg_fail_ydstogo'] = 10\n",
    "    df['fg_fail_game_seconds_remaining'] = np.maximum(0, df['game_seconds_remaining'] - 5)\n",
    "    df['fg_fail_half_seconds_remaining'] = np.maximum(0, df['half_seconds_remaining'] - 5)\n",
    "    df['fg_fail_score_differential'] = -df['score_differential']\n",
    "    df['fg_fail_posteam_timeouts_remaining'] = df['defteam_timeouts_remaining']\n",
    "    df['fg_fail_defteam_timeouts_remaining'] = df['posteam_timeouts_remaining']\n",
    "    df['fg_fail_score_time_ratio'] = df['fg_fail_score_differential'].abs() / (df['fg_fail_game_seconds_remaining'] + 1)\n",
    "    df['fg_fail_temp_F'] = df['temp_F']\n",
    "    df['fg_fail_wind_mph'] = df['wind_mph']\n",
    "    df['fg_fail_spread_line'] = df['spread_line']\n",
    "    df['fg_fail_total_line'] = df['total_line']\n",
    "    \n",
    "    # Next state if successful conversion attempt\n",
    "    df['go_success_yardline_100'] = df['yardline_100'] - df['ydstogo'] - 1 # Assume advancement to 1 yd beyond line to gain\n",
    "    df['go_success_down'] = 1\n",
    "    df['go_success_ydstogo'] = np.minimum(10, df['go_success_yardline_100'])\n",
    "    df['go_success_game_seconds_remaining'] = np.maximum(0, df['game_seconds_remaining'] - 5)\n",
    "    df['go_success_half_seconds_remaining'] = np.maximum(0, df['half_seconds_remaining'] - 5)\n",
    "    df['go_success_score_differential'] = df['score_differential']\n",
    "    df['go_success_posteam_timeouts_remaining'] = df['posteam_timeouts_remaining']\n",
    "    df['go_success_defteam_timeouts_remaining'] = df['defteam_timeouts_remaining']\n",
    "    df['go_success_score_time_ratio'] = df['go_success_score_differential'].abs() / (df['go_success_game_seconds_remaining'] + 1)\n",
    "    df['go_success_temp_F'] = df['temp_F']\n",
    "    df['go_success_wind_mph'] = df['wind_mph']\n",
    "    df['go_success_spread_line'] = df['spread_line']\n",
    "    df['go_success_total_line'] = df['total_line']\n",
    "    \n",
    "    # Next state if failed conversion attempt\n",
    "    df['go_fail_yardline_100'] = 100 - df['yardline_100']\n",
    "    df['go_fail_down'] = 1\n",
    "    df['go_fail_ydstogo'] = 10\n",
    "    df['go_fail_game_seconds_remaining'] = np.maximum(0, df['game_seconds_remaining'] - 5)\n",
    "    df['go_fail_half_seconds_remaining'] = np.maximum(0, df['half_seconds_remaining'] - 5)\n",
    "    df['go_fail_score_differential'] = -df['score_differential']\n",
    "    df['go_fail_posteam_timeouts_remaining'] = df['defteam_timeouts_remaining']\n",
    "    df['go_fail_defteam_timeouts_remaining'] = df['posteam_timeouts_remaining']\n",
    "    df['go_fail_score_time_ratio'] = df['go_fail_score_differential'].abs() / (df['go_fail_game_seconds_remaining'] + 1)\n",
    "    df['go_fail_temp_F'] = df['temp_F']\n",
    "    df['go_fail_wind_mph'] = df['wind_mph']\n",
    "    df['go_fail_spread_line'] = df['spread_line']\n",
    "    df['go_fail_total_line'] = df['total_line']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a270cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ewp_fg(df):\n",
    "    \n",
    "    if \"down\" in df.columns:\n",
    "        fourth_down_mask = df[\"down\"] == 4\n",
    "    else:\n",
    "        fourth_down_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    success_cols = [f\"fg_success_{f}\" for f in wp_features]\n",
    "    fail_cols    = [f\"fg_fail_{f}\"    for f in wp_features]\n",
    "\n",
    "    X_fg_success = df.loc[fourth_down_mask, success_cols].copy()\n",
    "    X_fg_success.columns = wp_features\n",
    "\n",
    "    X_fg_fail = df.loc[fourth_down_mask, fail_cols].copy()\n",
    "    X_fg_fail.columns = wp_features\n",
    "    \n",
    "    wp_fg_success = 1 - wp_symmetric_adjust(X_fg_success, predict_wp)\n",
    "    wp_fg_fail = 1 - wp_symmetric_adjust(X_fg_fail, predict_wp)\n",
    "\n",
    "    # Predict FG make probability using current state\n",
    "    X_fg_current = df.loc[fourth_down_mask, fg_features].copy()\n",
    "    p_make = fg_model.predict_proba(X_fg_current)[:, 1]\n",
    "    yardlines = X_fg_current['yardline_100']\n",
    "    p_make_decayed = np.where(yardlines >= (fg_decay_threshold - 17), p_make * np.maximum(0, (max_fg - 17 - yardlines) / (max_fg - fg_decay_threshold)), p_make)\n",
    "\n",
    "    # Compute expected WP for FG attempt\n",
    "    ewp_fg = np.full(len(df), np.nan)\n",
    "    ewp_fg[fourth_down_mask] = np.clip(p_make_decayed * wp_fg_success + (1 - p_make_decayed) * wp_fg_fail, 0, 1)\n",
    "\n",
    "    wp_fg_success_array = np.full(len(df), np.nan)\n",
    "    wp_fg_success_array[fourth_down_mask] = wp_fg_success\n",
    "\n",
    "    wp_fg_fail_array = np.full(len(df), np.nan)\n",
    "    wp_fg_fail_array[fourth_down_mask] = wp_fg_fail\n",
    "\n",
    "    # Save to DataFrame\n",
    "    df[\"ewp_fg\"] = ewp_fg\n",
    "    df['wp_fg_success'] = wp_fg_success_array\n",
    "    df['wp_fg_fail'] = wp_fg_fail_array\n",
    "    df[\"p_make_fg\"] = 0\n",
    "    df.loc[fourth_down_mask, \"p_make_fg\"] = p_make_decayed\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d6e8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ewp_go(df):\n",
    "    \n",
    "    if \"down\" in df.columns:\n",
    "        fourth_down_mask = df[\"down\"] == 4\n",
    "    else:\n",
    "        fourth_down_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    # Build success/fail WP feature frames\n",
    "    success_cols = [f\"go_success_{f}\" for f in wp_features]\n",
    "    fail_cols    = [f\"go_fail_{f}\"    for f in wp_features]\n",
    "\n",
    "    X_go_success = df.loc[fourth_down_mask, success_cols].copy()\n",
    "    X_go_success.columns = wp_features\n",
    "\n",
    "    X_go_fail = df.loc[fourth_down_mask, fail_cols].copy()\n",
    "    X_go_fail.columns = wp_features\n",
    "\n",
    "    # Raw state WPs (arrays aligned to fourth_down_mask rows)\n",
    "    wp_go_success = wp_symmetric_adjust(X_go_success, predict_wp)\n",
    "    wp_go_fail    = 1 - wp_symmetric_adjust(X_go_fail, predict_wp)\n",
    "\n",
    "    # Conversion probabilities\n",
    "    X_go_current = df.loc[fourth_down_mask, go_features].copy()\n",
    "    p_convert = go_model.predict_proba(X_go_current)[:, 1]\n",
    "\n",
    "    # Raw EWP\n",
    "    ewp_go = np.clip(\n",
    "        p_convert * wp_go_success + (1.0 - p_convert) * wp_go_fail,\n",
    "        0.0, 1.0\n",
    "    )\n",
    "\n",
    "    # Write back\n",
    "    df.loc[fourth_down_mask, \"p_convert\"] = p_convert\n",
    "    df.loc[fourth_down_mask, \"ewp_go\"] = ewp_go\n",
    "    df.loc[fourth_down_mask, \"wp_go_success\"] = wp_go_success\n",
    "    df.loc[fourth_down_mask, \"wp_go_fail\"] = wp_go_fail\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d28bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_punt_next_state(df):\n",
    "    \n",
    "    if \"down\" in df.columns:\n",
    "        fourth_down_mask = df[\"down\"] == 4\n",
    "    else:\n",
    "        fourth_down_mask = pd.Series(True, index=df.index)\n",
    "        \n",
    "    X_punt_current = df.loc[fourth_down_mask, punt_features]\n",
    "\n",
    "    X_punt_np = X_punt_current.values.astype(np.float32)\n",
    "    X_punt_np_scaled = X_scaler.transform(X_punt_np) # Scale inputs\n",
    "    X_punt_tensor = torch.tensor(X_punt_np_scaled, dtype=torch.float32)\n",
    "\n",
    "    # Predict (scaled output)\n",
    "    punt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_scaled_pred = punt_model(X_punt_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "    # Inverse transform target\n",
    "    punt_pred_yards = y_scaler.inverse_transform(\n",
    "        y_scaled_pred.reshape(-1, 1)\n",
    "    ).ravel()\n",
    "\n",
    "    punt_preds = np.zeros(len(df))\n",
    "    punt_preds[fourth_down_mask] = punt_pred_yards\n",
    "    \n",
    "    df['punt_pred_yards'] = punt_preds\n",
    " \n",
    "    landing_kicking = df['yardline_100'] - df['punt_pred_yards']\n",
    "    landing_kicking = np.where(landing_kicking < 0, 20, landing_kicking)  # Only clip beyond goal line\n",
    "    df['post_punt_yardline_100'] = 100 - landing_kicking # Flip field\n",
    "\n",
    "    df['post_punt_down'] = 1\n",
    "    df['post_punt_ydstogo'] = 10\n",
    "    df['post_punt_game_seconds_remaining'] = np.maximum(0, df['game_seconds_remaining'] - 8)\n",
    "    df['post_punt_half_seconds_remaining'] = np.maximum(0, df['half_seconds_remaining'] - 8)\n",
    "    df['post_punt_score_differential'] = -df['score_differential']\n",
    "    df['post_punt_posteam_timeouts_remaining'] = df['defteam_timeouts_remaining']\n",
    "    df['post_punt_defteam_timeouts_remaining'] = df['posteam_timeouts_remaining']\n",
    "    df['post_punt_score_time_ratio'] = df['post_punt_score_differential'].abs() / (df['post_punt_game_seconds_remaining'] + 1)\n",
    "    df['post_punt_temp_F'] = df['temp_F']\n",
    "    df['post_punt_wind_mph'] = df['wind_mph']\n",
    "    df['post_punt_spread_line'] = df['spread_line']\n",
    "    df['post_punt_total_line'] = df['total_line']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29d60321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ewp_punt(df):\n",
    "    \n",
    "    if \"down\" in df.columns:\n",
    "        fourth_down_mask = df[\"down\"] == 4\n",
    "    else:\n",
    "        fourth_down_mask = pd.Series(True, index=df.index)\n",
    "\n",
    "    post_cols = [f\"post_punt_{f}\" for f in wp_features]\n",
    "\n",
    "    X_post_punt = df.loc[fourth_down_mask, post_cols].copy()\n",
    "    X_post_punt.columns = wp_features\n",
    "    \n",
    "    wp_post_punt = 1 - wp_symmetric_adjust(X_post_punt, predict_wp)\n",
    "\n",
    "    # Compute expected WP for FG attempt\n",
    "    ewp_punt = np.full(len(df), np.nan)\n",
    "    ewp_punt[fourth_down_mask] = wp_post_punt\n",
    "\n",
    "    # Save to DataFrame\n",
    "    df[\"ewp_punt\"] = ewp_punt\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "903c7216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(df, test=False):\n",
    "    \n",
    "    ewp_cols = [\"ewp_punt\", \"ewp_fg\", \"ewp_go\"]\n",
    "    \n",
    "    if not test:\n",
    "\n",
    "        # Compute actual EWP for each row (using actual_ewp_col)\n",
    "        bad = set(df[\"actual_ewp_col\"].dropna().unique()) - set(df.columns)\n",
    "        if bad:\n",
    "            raise KeyError(f\"actual_ewp_col points to missing columns: {bad}\")\n",
    "\n",
    "        col_idx = df[[\"actual_ewp_col\"]].apply(\n",
    "            lambda x: df.columns.get_loc(x[0]),\n",
    "            axis=1\n",
    "        ).to_numpy()\n",
    "\n",
    "        row_idx = np.arange(len(df))\n",
    "        df[\"ewp_actual\"] = df.to_numpy()[row_idx, col_idx]\n",
    "\n",
    "    # Compute best EWP\n",
    "    df[\"ewp_best\"] = df[ewp_cols].max(axis=1)\n",
    "\n",
    "    # Mask\n",
    "    valid = (df[\"down\"] == 4) & df[ewp_cols].notna().all(axis=1)\n",
    "\n",
    "    # decision margin only for valid rows (avoids weird NaNs)\n",
    "    df[\"decision_margin\"] = np.nan\n",
    "    ewp_sorted = np.sort(df.loc[valid, ewp_cols].values, axis=1)\n",
    "    df.loc[valid, \"decision_margin\"] = ewp_sorted[:, -1] - ewp_sorted[:, -2]\n",
    "    \n",
    "    # go_margin: go vs best alternative\n",
    "    df[\"go_margin\"] = np.nan\n",
    "    df.loc[valid, \"go_margin\"] = (\n",
    "        df.loc[valid, \"ewp_go\"]\n",
    "        - df.loc[valid, [\"ewp_punt\", \"ewp_fg\"]].max(axis=1)\n",
    "    )\n",
    "    \n",
    "    col_to_action = {\n",
    "        \"ewp_punt\": \"punt\",\n",
    "        \"ewp_fg\": \"field_goal\",\n",
    "        \"ewp_go\": \"go\"\n",
    "    }\n",
    "\n",
    "    # determine best_col and recommended_play only for valid rows\n",
    "    df[\"best_col\"] = np.nan\n",
    "    df.loc[valid, \"best_col\"] = df.loc[valid, ewp_cols].idxmax(axis=1)\n",
    "    df[\"recommended_play\"] = np.nan\n",
    "    if valid.any():\n",
    "        df.loc[valid, \"recommended_play\"] = (\n",
    "            df.loc[valid, ewp_cols].idxmax(axis=1).map(col_to_action)\n",
    "        )\n",
    "    \n",
    "    # For cases where we know play_type_actual\n",
    "    if not test:\n",
    "        df[\"regret_actual\"] = pd.to_numeric(df[\"ewp_best\"] - df[\"ewp_actual\"], errors=\"coerce\")\n",
    "        \n",
    "        # Identify disagreement (only meaningful when recommendation exists)\n",
    "        df[\"disagreed\"] = np.nan\n",
    "        df.loc[valid, \"disagreed\"] = ~(\n",
    "            ((df.play_type_actual == \"punt\") & (df.recommended_play == \"punt\")) |\n",
    "            ((df.play_type_actual == \"field_goal\") & (df.recommended_play == \"field_goal\")) |\n",
    "            ((df.play_type_actual == \"go\") & (df.recommended_play == \"go\"))\n",
    "        )\n",
    "\n",
    "        df[\"follow_model\"] = np.nan\n",
    "        df.loc[valid, \"follow_model\"] = (df.loc[valid, \"actual_ewp_col\"] == df.loc[valid, \"best_col\"]).astype(int)\n",
    "       \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db73c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_state(pbp_fourth, best_params=None, nd=4):\n",
    "    r = pbp_fourth.iloc[0]\n",
    "\n",
    "    # --- Core quantities\n",
    "    wp_current = float(r.wp_current)\n",
    "    p_convert  = float(r.p_convert)\n",
    "\n",
    "    wp_succ = float(r.wp_go_success)\n",
    "    wp_fail = float(r.wp_go_fail)\n",
    "\n",
    "    # --- Deltas vs current\n",
    "    go_delta = float(r.ewp_go - wp_current)\n",
    "    fg_delta     = float(r.ewp_fg     - wp_current)\n",
    "    punt_delta   = float(r.ewp_punt   - wp_current)\n",
    "\n",
    "    # =========================\n",
    "    print(\"\\nTOPLINE\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"wp_current                    : {wp_current:.{nd}f}\")\n",
    "    print(f\"recommended_play              : {r.recommended_play}\")\n",
    "    print(f\"decision_margin               : {float(r.decision_margin):.{nd}f}\")\n",
    "\n",
    "    print(\"\\nEXPECTED WIN PROBABILITIES\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"go                            : {float(r.ewp_go):.{nd}f}   (Δ: {go_delta:+.{nd}f})\")\n",
    "    print(f\"field goal                    : {float(r.ewp_fg):.{nd}f}   (Δ: {fg_delta:+.{nd}f})\")\n",
    "    print(f\"punt                          : {float(r.ewp_punt):.{nd}f}   (Δ: {punt_delta:+.{nd}f})\")\n",
    "\n",
    "    print(\"\\nGO DETAILS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"p_convert                     : {p_convert:.{nd}f}\")\n",
    "    print(f\"wp_success                    : {wp_succ:.{nd}f}\")\n",
    "    print(f\"wp_fail                       : {wp_fail:.{nd}f} → {wp_fail:.{nd}f}\")\n",
    "\n",
    "    print(\"\\nFG DETAILS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"p_make_fg                     : {float(r.p_make_fg):.{nd}f}\")\n",
    "    print(f\"wp_success                    : {float(r.wp_fg_success):.{nd}f}\")\n",
    "    print(f\"wp_fail                       : {float(r.wp_fg_fail):.{nd}f}\")\n",
    "\n",
    "    print(\"\\nPUNT CONTEXT\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"predicted net punt yds        : {float(r.punt_pred_yards):.{nd}f}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "260c88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_with_ewp(df, test=False):\n",
    "    \n",
    "    pbp_pre_computed = df.copy()\n",
    "    pbp_pre_computed[\"wp_pred\"] = wp_symmetric_adjust(pbp_pre_computed, predict_wp)\n",
    "\n",
    "    # Outcomes only exist for real data\n",
    "    if not test:\n",
    "        pbp_pre_computed = create_plays_df(pbp_pre_computed)\n",
    "\n",
    "    # shared features\n",
    "    pbp_pre_computed[\"score_time_ratio\"] = (pbp_pre_computed[\"score_differential\"].abs() / (pbp_pre_computed[\"game_seconds_remaining\"] + 1))\n",
    "    \n",
    "    # current-state WP\n",
    "    pbp_pre_computed[\"wp_current\"] = wp_symmetric_adjust(\n",
    "        pbp_pre_computed[wp_features], predict_wp\n",
    "    )\n",
    "\n",
    "    # EWP components (these should internally compute their own mask based on df[\"down\"] == 4)\n",
    "    pbp_pre_computed = create_next_fg_conv_states(pbp_pre_computed)\n",
    "    pbp_pre_computed = calculate_ewp_fg(pbp_pre_computed)\n",
    "    pbp_pre_computed = calculate_ewp_go(pbp_pre_computed)\n",
    "    pbp_pre_computed = create_punt_next_state(pbp_pre_computed)\n",
    "    pbp_pre_computed = calculate_ewp_punt(pbp_pre_computed)\n",
    "\n",
    "    # recommendations/regret/follow_model/etc.\n",
    "    pbp_pre_computed = make_recommendations(pbp_pre_computed, test=test)\n",
    "    pbp_fourth = pbp_pre_computed[pbp_pre_computed.down == 4].copy()\n",
    "    \n",
    "    if test:\n",
    "        report_state(pbp_fourth)\n",
    "\n",
    "    return pbp_pre_computed, pbp_fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0cf0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp_pre_train, pbp_fourth_train = create_df_with_ewp(pbp_train)\n",
    "pbp_pre_test, pbp_fourth_test = create_df_with_ewp(pbp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afa437a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOPLINE\n",
      "----------------------------------------\n",
      "wp_current                    : 0.4736\n",
      "recommended_play              : field_goal\n",
      "decision_margin               : 0.0298\n",
      "\n",
      "EXPECTED WIN PROBABILITIES\n",
      "----------------------------------------\n",
      "go                            : 0.4602   (Δ: -0.0133)\n",
      "field goal                    : 0.4901   (Δ: +0.0165)\n",
      "punt                          : 0.4021   (Δ: -0.0715)\n",
      "\n",
      "GO DETAILS\n",
      "----------------------------------------\n",
      "p_convert                     : 0.4794\n",
      "wp_success                    : 0.5409\n",
      "wp_fail                       : 0.3859 → 0.3859\n",
      "\n",
      "FG DETAILS\n",
      "----------------------------------------\n",
      "p_make_fg                     : 0.8999\n",
      "wp_success                    : 0.5023\n",
      "wp_fail                       : 0.3799\n",
      "\n",
      "PUNT CONTEXT\n",
      "----------------------------------------\n",
      "predicted net punt yds        : 9.1657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "  \"yardline_100\": 15,\n",
    "  \"down\": 4,\n",
    "  \"ydstogo\": 4,\n",
    "  \"game_seconds_remaining\": 2200,\n",
    "  \"half_seconds_remaining\": 400,\n",
    "  \"score_differential\": -3,\n",
    "  \"posteam_timeouts_remaining\": 3,\n",
    "  \"defteam_timeouts_remaining\": 3,\n",
    "  \"temp_F\": 30,\n",
    "  \"wind_mph\": 10,\n",
    "  \"spread_line\": 0,\n",
    "  \"total_line\": 45\n",
    "}\n",
    "\n",
    "test_state = pd.DataFrame([state])\n",
    "test_pbp_pre_computed, test_pbp_fourth = create_df_with_ewp(test_state, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e1c37db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exports/punt_model_20260114_1744.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Variables\n",
    "joblib.dump(test_season, f\"exports/test_season_{timestamp}.joblib\")\n",
    "joblib.dump(wp_features, f\"exports/wp_features_{timestamp}.joblib\")\n",
    "joblib.dump(go_features, f\"exports/go_features_{timestamp}.joblib\")\n",
    "joblib.dump(fg_features, f\"exports/fg_features_{timestamp}.joblib\")\n",
    "joblib.dump(punt_features, f\"exports/punt_features_{timestamp}.joblib\")\n",
    "\n",
    "# Dataframes\n",
    "raw_pbp.to_parquet(f\"exports/raw_pbp{timestamp}.parquet\")\n",
    "pbp.to_parquet(f\"exports/pbp{timestamp}.parquet\")\n",
    "pbp_fourth_train.to_parquet(f\"exports/pbp_fourth_train_{timestamp}.parquet\")\n",
    "pbp_fourth_test.to_parquet(f\"exports/pbp_fourth_test_{timestamp}.parquet\")\n",
    "\n",
    "# Models\n",
    "joblib.dump(wp_model, f\"exports/wp_model_{timestamp}.joblib\")\n",
    "joblib.dump(go_model, f\"exports/go_model_{timestamp}.joblib\")\n",
    "joblib.dump(fg_model, f\"exports/fg_model_{timestamp}.joblib\")\n",
    "joblib.dump(punt_model, f\"exports/punt_model_{timestamp}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6c180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
