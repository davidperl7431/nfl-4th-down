{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1336a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on path\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from project_code.functions import *\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20734c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2789a046d90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7e2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading timestamp: 20260202_2252\n"
     ]
    }
   ],
   "source": [
    "exports = Path(\"../exports\")\n",
    "\n",
    "def latest_timestamp(prefix=\"wp_model_\", suffix=\".joblib\"):\n",
    "    pat = re.compile(rf\"^{re.escape(prefix)}(\\d{{8}}_\\d{{4}}){re.escape(suffix)}$\")\n",
    "    ts_list = []\n",
    "    for p in exports.iterdir():\n",
    "        m = pat.match(p.name)\n",
    "        if m:\n",
    "            ts_list.append(m.group(1))\n",
    "    if not ts_list:\n",
    "        raise FileNotFoundError(f\"No files matching {prefix}YYYYMMDD_HHMM{suffix} in {exports.resolve()}\")\n",
    "    return sorted(ts_list)[-1]\n",
    "\n",
    "ts = latest_timestamp(prefix=\"wp_model_\", suffix=\".joblib\")\n",
    "print(\"Loading timestamp:\", ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Variables ----------\n",
    "test_season   = joblib.load(f\"../exports/test_season_{ts}.joblib\")\n",
    "wp_features   = joblib.load(f\"../exports/wp_features_{ts}.joblib\")\n",
    "wp_base_features   = joblib.load(f\"../exports/wp_base_features_{ts}.joblib\")\n",
    "go_features   = joblib.load(f\"../exports/go_features_{ts}.joblib\")\n",
    "fg_features   = joblib.load(f\"../exports/fg_features_{ts}.joblib\")\n",
    "punt_features = joblib.load(f\"../exports/punt_features_{ts}.joblib\")\n",
    "fg_features   = joblib.load(f\"../exports/fg_features_{ts}.joblib\")\n",
    "go_folds = joblib.load(f\"../exports/go_folds_{ts}.joblib\")\n",
    "\n",
    "# ---------- DataFrames ----------\n",
    "raw_pbp = pd.read_parquet(f\"../exports/raw_pbp{ts}.parquet\")\n",
    "pbp = pd.read_parquet(f\"../exports/pbp{ts}.parquet\")\n",
    "pbp_fourth_train = pd.read_parquet(f\"../exports/pbp_fourth_train_{ts}.parquet\")\n",
    "pbp_fourth_test  = pd.read_parquet(f\"../exports/pbp_fourth_test_{ts}.parquet\")\n",
    "\n",
    "# ---------- Models ----------\n",
    "wp_model   = joblib.load(f\"../exports/wp_model_{ts}.joblib\")\n",
    "go_model   = joblib.load(f\"../exports/go_model_{ts}.joblib\")\n",
    "fg_model   = joblib.load(f\"../exports/fg_model_{ts}.joblib\")\n",
    "punt_model = joblib.load(f\"../exports/punt_model_{ts}.joblib\")\n",
    "\n",
    "# Objects\n",
    "X_scaler = joblib.load(f\"../exports/X_scaler_{ts}.joblib\")\n",
    "y_scaler = joblib.load(f\"../exports/y_scaler_{ts}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ade70",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "  \"yardline_100\": 4,\n",
    "  \"down\": 4,\n",
    "  \"ydstogo\": 4,\n",
    "  \"game_seconds_remaining\": 1250,\n",
    "  \"half_seconds_remaining\": 1250,\n",
    "  \"score_differential\": 0,\n",
    "  \"posteam_timeouts_remaining\": 2,\n",
    "  \"defteam_timeouts_remaining\": 2,\n",
    "  \"temp_F\": 20,\n",
    "  \"wind_mph\": 10,\n",
    "  \"possession_spread_line\": -4.5,\n",
    "  \"total_line\": 43.5\n",
    "}\n",
    "\n",
    "test_results, fourth_test_results = create_df_with_ewp(\n",
    "    pd.DataFrame([state]),\n",
    "    wp_model=wp_model,\n",
    "    go_model=go_model,\n",
    "    fg_model=fg_model,\n",
    "    punt_model=punt_model,\n",
    "    wp_features=wp_features,\n",
    "    wp_base_features=wp_base_features,\n",
    "    go_features=go_features,\n",
    "    fg_features=fg_features,\n",
    "    punt_features=punt_features,\n",
    "    test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06cfb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration + Brier + Murphy decomposition\n",
    "y = pbp_fourth_test[\"win_actual\"].astype(float).to_numpy()\n",
    "p = pbp_fourth_test[\"wp_pred\"].astype(float).to_numpy()\n",
    "\n",
    "# Bin predicted probabilities\n",
    "bins = np.linspace(0, 1, 11)  # 10 equal-width bins\n",
    "wp_bin = pd.cut(pbp_fourth_test[\"wp_pred\"], bins=bins, include_lowest=True)\n",
    "\n",
    "# Aggregate by bin\n",
    "cal_table = (\n",
    "    pbp_fourth_test\n",
    "    .groupby(wp_bin, observed=False)\n",
    "    .agg(\n",
    "        wp_pred_mean=(\"wp_pred\", \"mean\"),\n",
    "        win_rate=(\"win_actual\", \"mean\"),\n",
    "        count=(\"win_actual\", \"count\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "N = len(pbp_fourth_test)\n",
    "p_bar = cal_table[\"wp_pred_mean\"].to_numpy(dtype=float)\n",
    "o_bar = cal_table[\"win_rate\"].to_numpy(dtype=float)\n",
    "n_k  = cal_table[\"count\"].to_numpy(dtype=float)\n",
    "\n",
    "o = y.mean()\n",
    "\n",
    "reliability = np.sum((n_k / N) * (p_bar - o_bar) ** 2)\n",
    "resolution  = np.sum((n_k / N) * (o_bar - o) ** 2)\n",
    "uncertainty = o * (1.0 - o)\n",
    "\n",
    "brier = reliability - resolution + uncertainty\n",
    "\n",
    "print(f\"Brier score:  {brier:.5f}\")\n",
    "print(f\"Reliability:  {reliability:.5f}\")\n",
    "print(f\"Resolution:   {resolution:.5f}\")\n",
    "print(f\"Uncertainty:  {uncertainty:.5f}\")\n",
    "\n",
    "x = cal_table[\"wp_pred_mean\"].to_numpy()\n",
    "yhat = cal_table[\"win_rate\"].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(x, yhat, marker=\"o\")\n",
    "plt.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "plt.xlabel(\"Predicted WP\")\n",
    "plt.ylabel(\"Observed Win Rate\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1212fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = pbp_fourth_test[wp_features].copy()\n",
    "\n",
    "state_flipped = state.copy()\n",
    "state_flipped[\"score_differential\"] *= -1\n",
    "state_flipped[[\"posteam_timeouts_remaining\",\"defteam_timeouts_remaining\"]] = (\n",
    "    state_flipped[[\"defteam_timeouts_remaining\",\"posteam_timeouts_remaining\"]].values\n",
    ")\n",
    "state_flipped[\"yardline_100\"] = 100 - state_flipped[\"yardline_100\"]\n",
    "state_flipped[\"possession_spread_line\"] *= -1\n",
    "\n",
    "wp_raw = predict_wp(state, wp_model, wp_features)\n",
    "wp_raw_flipped = predict_wp(state_flipped, wp_model, wp_features)\n",
    "\n",
    "flip_err = wp_raw + wp_raw_flipped - 1\n",
    "print(\"flip_err mean:\", flip_err.mean())\n",
    "print(\"flip_err abs mean:\", np.abs(flip_err).mean())\n",
    "print(\"flip_err std:\", flip_err.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 11)\n",
    "calibration_df = pd.DataFrame(index=pd.IntervalIndex.from_tuples([(round(bins[i],2), round(bins[i+1],2)) for i in range(len(bins)-1)]))\n",
    "\n",
    "# Function to compute empirical win fraction per bin\n",
    "def empirical_win_fraction(pred_col):\n",
    "    return pbp_fourth_test.groupby(pd.cut(pbp_fourth_test[pred_col], bins=bins))['win_actual'].mean()\n",
    "\n",
    "# Compute results for each play type\n",
    "calibration_df['ewp_punt'] = round(empirical_win_fraction('ewp_punt'),3)\n",
    "calibration_df['ewp_fg'] = round(empirical_win_fraction('ewp_fg'),3)\n",
    "calibration_df['ewp_go'] = round(empirical_win_fraction('ewp_go'),3)\n",
    "\n",
    "# Bins indicate predicted wp bin; columns are how often a team actually won in that predicted wp bin\n",
    "calibration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ewp_columns = ['ewp_punt', 'ewp_fg', 'ewp_go']\n",
    "\n",
    "# Boolean mask of violations\n",
    "violation_mask = (pbp_fourth_test[ewp_columns] < 0) | (pbp_fourth_test[ewp_columns] > 1)\n",
    "\n",
    "# Count violations per column\n",
    "violations = violation_mask.sum()\n",
    "\n",
    "if violations.sum() == 0:\n",
    "    print(\"No EWP violations detected.\")\n",
    "else:\n",
    "    print(\"Violations detected:\")\n",
    "    print(violations)\n",
    "    print(pbp_fourth_test[violation_mask.any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ed58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional mean regret\n",
    "pbp_fourth_test[pbp_fourth_test.follow_model == 0].regret_actual.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute regret stats by play type ---\n",
    "df_disagree = pbp_fourth_test[pbp_fourth_test[\"disagreed\"] == True].copy()\n",
    "regret_by_play = df_disagree.groupby('play_type_actual')['regret_actual'].agg(['mean', 'median'])\n",
    "df_disagree['regret_actual'] = pd.to_numeric(df_disagree['regret_actual'], errors='coerce')\n",
    "\n",
    "# Compute stats\n",
    "regret_by_play = df_disagree.groupby('play_type_actual')['regret_actual'].agg(['size', 'mean', 'median'])\n",
    "regret_by_play['95th'] = df_disagree.groupby('play_type_actual')['regret_actual'].quantile(0.95)\n",
    "\n",
    "print(\"Regret by Play Type Conditioned on Coach Disagreement:\")\n",
    "regret_by_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regret vs margin for disagreements\n",
    "disagree_bins = pd.qcut(df_disagree[\"decision_margin\"], q=10, duplicates=\"drop\")\n",
    "regret_by_margin = df_disagree.groupby(disagree_bins)[\"regret_actual\"].agg(\n",
    "    mean=\"mean\",\n",
    "    p95=lambda x: np.percentile(x, 95),\n",
    "    count=\"count\"\n",
    ")\n",
    "print(\"Regret vs Decision Margin (Disagreements)\")\n",
    "display(regret_by_margin)\n",
    "\n",
    "# Bin decision margin\n",
    "pbp_fourth_test[\"margin_bin\"] = pd.qcut(pbp_fourth_test[\"decision_margin\"], 10)\n",
    "follow_by_margin = pbp_fourth_test.groupby(\"margin_bin\").agg(\n",
    "    follow_rate=(\"follow_model\", \"mean\"),\n",
    "    count=(\"follow_model\", \"size\")\n",
    ")\n",
    "print(\"Follow Model Rate vs Decision Margin\")\n",
    "follow_by_margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc217968",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pbp_fourth_test[pbp_fourth_test.season == test_season]\n",
    "\n",
    "labels = [\"punt\", \"field_goal\", \"go\"]\n",
    "\n",
    "cm_df = test[[\"play_type_actual\", \"recommended_play\"]].dropna()\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    cm_df[\"play_type_actual\"].astype(str),\n",
    "    cm_df[\"recommended_play\"].astype(str),\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "confusion = pd.DataFrame(\n",
    "    cm,\n",
    "    index=pd.Index(labels, name=\"Actual\"),\n",
    "    columns=pd.Index(labels, name=\"Recommended\")\n",
    ")\n",
    "\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_norm = confusion.div(confusion.sum(axis=1), axis=0)\n",
    "round(confusion_norm,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_go_from_margin(margin, scale):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-margin / scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544cd88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_scale_cv(df, folds, scale_grid):\n",
    "    \n",
    "    losses = []\n",
    "    for s in scale_grid:\n",
    "        fold_losses = []\n",
    "        for tr_idx, va_idx in folds:\n",
    "            dval = df.iloc[va_idx]\n",
    "            y = (dval[\"play_type_actual\"] == \"go\").astype(int).to_numpy()\n",
    "            p = p_go_from_margin(dval[\"go_margin\"].to_numpy(), scale=s)\n",
    "            p = np.clip(p, 1e-6, 1 - 1e-6)\n",
    "            fold_losses.append(log_loss(y, p))\n",
    "        losses.append((s, float(np.mean(fold_losses))))\n",
    "    losses.sort(key=lambda x: x[1])\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3931acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_grid = np.geomspace(0.002, 0.05, 30)\n",
    "losses = tune_scale_cv(pbp_fourth_train.reset_index(drop=True), go_folds, scale_grid)\n",
    "\n",
    "best_scale, best_loss = losses[0]\n",
    "print(best_scale, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd61e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pbp_fourth_test.copy()\n",
    "\n",
    "df[\"p_go_model\"] = p_go_from_margin(\n",
    "    df[\"go_margin\"],\n",
    "    scale=best_scale\n",
    ").clip(.01, .99)\n",
    "\n",
    "df[\"coach_go\"] = (\n",
    "    df[\"play_type_actual\"] == \"go\"\n",
    ").astype(int)\n",
    "\n",
    "df[\"go_diff\"] = df[\"coach_go\"] - df[\"p_go_model\"]\n",
    "\n",
    "league_bias = df[\"go_diff\"].mean()\n",
    "\n",
    "rai = (\n",
    "    df.groupby(\"possession_coach\")[\"go_diff\"].mean()\n",
    "    - league_bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pbp_fourth_test.copy()\n",
    "\n",
    "df[\"p_go_model\"] = p_go_from_margin(\n",
    "    df[\"go_margin\"],\n",
    "    scale=best_scale\n",
    ").clip(.01, .99)\n",
    "\n",
    "df[\"coach_go\"] = (\n",
    "    df[\"play_type_actual\"] == \"go\"\n",
    ").astype(int)\n",
    "\n",
    "\n",
    "df[\"go_diff\"] = (\n",
    "    df[\"coach_go\"]\n",
    "    - df[\"p_go_model\"]\n",
    ")\n",
    "\n",
    "league_bias = df[\"go_diff\"].mean()\n",
    "df[\"relative_go_diff\"] = df[\"go_diff\"] - league_bias\n",
    "\n",
    "rai_raw = (\n",
    "    df\n",
    "    .groupby(\"possession_coach\")\n",
    "    .agg(\n",
    "        rai=(\"relative_go_diff\", \"mean\"),\n",
    "        n=(\"relative_go_diff\", \"size\")\n",
    "    )\n",
    "    .sort_values(\"rai\")\n",
    ")\n",
    "\n",
    "k = 30  # prior strength\n",
    "rai_raw[\"rai_shrunk\"] = (\n",
    "    rai_raw[\"n\"] / (rai_raw[\"n\"] + k)\n",
    ") * rai_raw[\"rai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_raw.sort_values(\"rai_shrunk\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Safely adds derived football features.\n",
    "    Only creates features when required base columns exist.\n",
    "    Missing dependencies -> feature is created as NaN or 0.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if \"yardline_100\" in df.columns:\n",
    "        df[\"is_red_zone\"] = (df[\"yardline_100\"] <= 20).astype(int)\n",
    "    else:\n",
    "        df[\"is_red_zone\"] = 0\n",
    "\n",
    "    if {\"ydstogo\", \"yardline_100\"}.issubset(df.columns):\n",
    "        df[\"is_goal_to_go\"] = (df[\"ydstogo\"] >= df[\"yardline_100\"]).astype(int)\n",
    "    else:\n",
    "        df[\"is_goal_to_go\"] = 0\n",
    "\n",
    "    if \"ydstogo\" in df.columns:\n",
    "        df[\"log_ydstogo\"] = np.log1p(df[\"ydstogo\"].clip(lower=0))\n",
    "    else:\n",
    "        df[\"log_ydstogo\"] = np.nan\n",
    "\n",
    "    if \"game_seconds_remaining\" in df.columns:\n",
    "        df[\"log_game_seconds_remaining\"] = np.log1p(df[\"game_seconds_remaining\"].clip(lower=0))\n",
    "    else:\n",
    "        df[\"log_game_seconds_remaining\"] = np.nan\n",
    "\n",
    "    if \"score_differential\" in df.columns:\n",
    "        df[\"abs_score_differential\"] = df[\"score_differential\"].abs()\n",
    "    else:\n",
    "        df[\"abs_score_differential\"] = np.nan\n",
    "\n",
    "    if {\"score_differential\", \"game_seconds_remaining\"}.issubset(df.columns):\n",
    "        df[\"score_time_ratio\"] = (df[\"score_differential\"].abs() / (df[\"game_seconds_remaining\"] + 1))\n",
    "    else:\n",
    "        df[\"score_time_ratio\"] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_raw.sort_values(\"rai_shrunk\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ecb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "yard_edges  = list(range(1, 101, 10)) + [101]\n",
    "yard_labels = [f\"{start}–{start+9}\" for start in yard_edges[:-1]]\n",
    "\n",
    "df[\"yardline_bin\"] = pd.cut(\n",
    "    df[\"yardline_100\"],\n",
    "    bins=yard_edges,\n",
    "    labels=yard_labels,\n",
    "    right=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "\n",
    "togo_bins   = [0, 1, 3, 6, 9, np.inf]\n",
    "togo_labels = [\"1\", \"2–3\", \"4–6\", \"7–9\", \"10+\"]\n",
    "\n",
    "df[\"ydstogo_bin\"] = pd.cut(\n",
    "    df[\"ydstogo\"],\n",
    "    bins=togo_bins,\n",
    "    labels=togo_labels,\n",
    "    right=True,\n",
    "    include_lowest=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e30608",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = (\n",
    "    df\n",
    "    .groupby([\"ydstogo_bin\", \"yardline_bin\"], observed=False)[\"go_diff\"]\n",
    "    .mean()\n",
    "    .unstack(\"yardline_bin\")\n",
    ")\n",
    "\n",
    "counts = (\n",
    "    df\n",
    "    .groupby([\"ydstogo_bin\", \"yardline_bin\"], observed=False)[\"go_diff\"]\n",
    "    .size()\n",
    "    .unstack(\"yardline_bin\")\n",
    ")\n",
    "\n",
    "min_n = 10\n",
    "heat = heat.mask(counts < min_n)\n",
    "\n",
    "round(heat,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665fca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = df.groupby(\"possession_coach\").agg(\n",
    "    go_rate=(\"coach_go\", \"mean\"),\n",
    "    n=(\"coach_go\", \"size\")\n",
    ")\n",
    "\n",
    "rai = rai_raw.copy()\n",
    "\n",
    "rai[\"rai_shrunk\"] = (\n",
    "    rai[\"n\"] / (rai[\"n\"] + k)\n",
    ") * rai[\"rai\"]\n",
    "\n",
    "agg = (\n",
    "    df.groupby(\"possession_coach\")\n",
    "      .agg(\n",
    "          go_rate=(\"coach_go\", \"mean\"),\n",
    "          n=(\"coach_go\", \"size\")\n",
    "      )\n",
    ")\n",
    "\n",
    "rai_plot = (\n",
    "    agg\n",
    "    .join(rai_raw[[\"rai_shrunk\"]])\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "x0 = rai_plot[\"go_rate\"].mean()     # league-average aggression\n",
    "y0 = 0.0                            # neutral intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e54445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    rai_plot[\"go_rate\"],\n",
    "    rai_plot[\"rai_shrunk\"],\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.axvline(x0)\n",
    "plt.axhline(y0)\n",
    "\n",
    "# --- ADD LABELS\n",
    "for coach, r in rai_plot.iterrows():\n",
    "    plt.text(\n",
    "        r[\"go_rate\"],\n",
    "        r[\"rai_shrunk\"],\n",
    "        str(coach),\n",
    "        fontsize=8,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Aggression: Go-for-it rate\")\n",
    "plt.ylabel(\"Intelligence: RAI (shrunk)\")\n",
    "plt.title(\"Aggression vs Intelligence\")\n",
    "\n",
    "plt.text(\n",
    "    r[\"go_rate\"] + 0.002,\n",
    "    r[\"rai_shrunk\"] + 0.002,\n",
    "    str(coach),\n",
    "    fontsize=8,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbp_all, pbp_fourth_all = create_df_with_ewp(\n",
    "    pbp,\n",
    "    wp_model=wp_model,\n",
    "    go_model=go_model,\n",
    "    fg_model=fg_model,\n",
    "    punt_model=punt_model,\n",
    "    wp_features=wp_features,\n",
    "    wp_base_features=wp_base_features,\n",
    "    go_features=go_features,\n",
    "    fg_features=fg_features,\n",
    "    punt_features=punt_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pbp_fourth_all.copy()\n",
    "\n",
    "df[\"p_go_model\"] = (\n",
    "    p_go_from_margin(df[\"go_margin\"], scale=best_scale)\n",
    "    .clip(0.01, 0.99)\n",
    ")\n",
    "\n",
    "df[\"coach_go\"] = (df[\"play_type_actual\"] == \"go\").astype(int)\n",
    "\n",
    "# decision residual: actual go (0/1) minus model p(go)\n",
    "df[\"go_diff\"] = df[\"coach_go\"] - df[\"p_go_model\"]\n",
    "\n",
    "# league bias by season (important: the league's aggressiveness changes over time)\n",
    "league_bias_by_season = df.groupby(\"season\")[\"go_diff\"].mean()\n",
    "\n",
    "# RAI per team-season (mean residual, de-biased within season)\n",
    "rai_team_season = (\n",
    "    df.groupby([\"season\", \"posteam\"])[\"go_diff\"].mean()\n",
    "    .rename(\"rai_raw\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "rai_team_season[\"league_bias\"] = rai_team_season[\"season\"].map(league_bias_by_season)\n",
    "rai_team_season[\"rai\"] = rai_team_season[\"rai_raw\"] - rai_team_season[\"league_bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "past = (rai_team_season.query(\"season in [2022, 2023, 2024]\")\n",
    "        .groupby(\"posteam\")[\"rai\"]\n",
    "        .mean()\n",
    "        .rename(\"rai_22_24\"))\n",
    "\n",
    "future = (rai_team_season.query(\"season == 2025\")\n",
    "          .set_index(\"posteam\")[\"rai\"]\n",
    "          .rename(\"rai_2025\"))\n",
    "\n",
    "stick_df = past.to_frame().join(future, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da50681",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stick_df[\"rai_22_24\"].to_numpy()\n",
    "y = stick_df[\"rai_2025\"].to_numpy()\n",
    "\n",
    "pearson_r, pearson_p = pearsonr(x, y)\n",
    "spearman_r, spearman_p = spearmanr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# scatter\n",
    "plt.scatter(x, y, alpha=0.8)\n",
    "\n",
    "# regression line\n",
    "coef = np.polyfit(x, y, 1)\n",
    "x_line = np.linspace(x.min(), x.max(), 100)\n",
    "y_line = coef[0] * x_line + coef[1]\n",
    "plt.plot(x_line, y_line, lw=2)\n",
    "\n",
    "# reference lines\n",
    "plt.axhline(0, color=\"black\", lw=1, alpha=0.6)\n",
    "plt.axvline(0, color=\"black\", lw=1, alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Average RAI (2022–2024)\")\n",
    "plt.ylabel(\"RAI (2025)\")\n",
    "plt.title(\"Past RAI vs Future RAI (Out-of-Sample)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be79c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Update analysis and functions notebooks\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2da66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
